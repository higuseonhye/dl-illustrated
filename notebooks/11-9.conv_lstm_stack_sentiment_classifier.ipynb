{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 합성곱-LSTM 적층 감성 분류기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북에서 합성곱 층 위에 LSTM을 쌓아 감성에 따라 IMDB 영화 리뷰를 분류합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/dl-illustrated/blob/master/notebooks/11-9.conv_lstm_stack_sentiment_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 라이브러리 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.layers import Bidirectional \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 디렉토리\n",
    "output_dir = 'model_output/cnnLSTM'\n",
    "\n",
    "# 훈련\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "\n",
    "# 벡터 공간 임베딩\n",
    "n_dim = 64 \n",
    "n_unique_words = 10000 \n",
    "max_review_length = 200 \n",
    "pad_type = trunc_type = 'pre'\n",
    "drop_embed = 0.2 \n",
    "\n",
    "# 합성곱 층 구조\n",
    "n_conv = 64  \n",
    "k_conv = 3 \n",
    "mp_size = 4\n",
    "\n",
    "# LSTM 층 구조\n",
    "n_lstm = 64 \n",
    "drop_lstm = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = imdb.load_data(num_words=n_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_review_length, padding=pad_type, truncating=trunc_type, value=0)\n",
    "x_valid = pad_sequences(x_valid, maxlen=max_review_length, padding=pad_type, truncating=trunc_type, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 신경망 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) \n",
    "model.add(SpatialDropout1D(drop_embed))\n",
    "model.add(Conv1D(n_conv, k_conv, activation='relu'))\n",
    "model.add(MaxPooling1D(mp_size))\n",
    "model.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 64)           640000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 198, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 718,529\n",
      "Trainable params: 718,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 양 방향으로 역전파되기 때문에 LSTM 층의 파라미터가 두 배가 됩니다.\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "196/196 [==============================] - 8s 42ms/step - loss: 0.4535 - accuracy: 0.7579 - val_loss: 0.3062 - val_accuracy: 0.8730\n",
      "Epoch 2/4\n",
      "196/196 [==============================] - 7s 34ms/step - loss: 0.2344 - accuracy: 0.9099 - val_loss: 0.3191 - val_accuracy: 0.8623\n",
      "Epoch 3/4\n",
      "196/196 [==============================] - 7s 34ms/step - loss: 0.1704 - accuracy: 0.9374 - val_loss: 0.3417 - val_accuracy: 0.8635\n",
      "Epoch 4/4\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 0.1183 - accuracy: 0.9592 - val_loss: 0.3831 - val_accuracy: 0.8572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac284ad630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.02.hdf5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'94.36'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(roc_auc_score(y_valid, y_hat)*100.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.3 on Python 3.6 (CUDA 10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
